{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:45:10.993533Z",
     "start_time": "2024-06-10T13:45:10.989160Z"
    }
   },
   "outputs": [],
   "source": [
    "#!source /usr/local/Ascend/ascend-toolkit/set_env.sh\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from functools import partial\n",
    "from esm.modules import ContactPredictionHead, ESM1bLayerNorm, RobertaLMHead, TransformerLayer, MultiheadAttention \n",
    "import esm\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d99a535242f945a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:45:11.130618Z",
     "start_time": "2024-06-10T13:45:11.127760Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# print(f\"Number of NPU devices: {torch.npu.device_count()}\")\n",
    "# print(f\"Current NPU device index: {torch.npu.current_device()}\")\n",
    "# print(f\"NPU device name: {torch.npu.get_device_name(torch.npu.current_device())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "287e8411978aac68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:45:11.212788Z",
     "start_time": "2024-06-10T13:45:11.204799Z"
    }
   },
   "outputs": [],
   "source": [
    "class GroundingAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=4, qkv_bias=True,\n",
    "                 attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim**-0.5\n",
    "\n",
    "        self.kv = nn.Linear(dim, dim*2, bias=qkv_bias)\n",
    "        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        # self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x, r):\n",
    "        B, N, C = x.shape\n",
    "        B_, N_, C_ = r.shape\n",
    "\n",
    "        kv = self.kv(r).reshape(B_, N_, 2, self.num_heads, C_ //\n",
    "                                self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        k, v = kv.unbind(0)\n",
    "        q = self.q(x).reshape(B, N, self.num_heads, C //\n",
    "                              self.num_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale  # (B, heads, N, N_)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        # x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f3b0ecd2235e8eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:45:11.271420Z",
     "start_time": "2024-06-10T13:45:11.265777Z"
    }
   },
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(FFN, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        输入 x 的形状: (batch_size, seq_len, input_dim)\n",
    "        输出 y 的形状: (batch_size, seq_len, output_dim)\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        # x = x.view(-1, x.size(-1))  # 将 x 展平成二维\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        # x = x.view(batch_size, seq_len, -1)  # 将 x 恢复成三维\n",
    "        x = residual + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88281771-5ee7-449a-9d52-20ee8de54c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bertlayer(nn.Module):\n",
    "    def __init__(self, embeddingdim, hidden_dim, num_head = 16):\n",
    "        super(bertlayer, self).__init__()\n",
    "        self.atte_norm = ESM1bLayerNorm(embeddingdim)\n",
    "        self.ffn_norm = ESM1bLayerNorm(embeddingdim)\n",
    "        self.atte = torch.nn.MultiheadAttention(embed_dim = embeddingdim, num_heads = num_head, dropout = 0.0)\n",
    "        self.ffn = FFN(embeddingdim, hidden_dim)\n",
    "    def forward(self, x, x_padding_mask):\n",
    "        residual = x\n",
    "        x = self.atte_norm(x)\n",
    "        x, _ = self.atte(x, x, x, key_padding_mask = x_padding_mask)\n",
    "        x = x + residual\n",
    "        x = x + self.ffn(self.ffn_norm(x))\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "769fd10418ae2c28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:45:11.336232Z",
     "start_time": "2024-06-10T13:45:11.321388Z"
    }
   },
   "outputs": [],
   "source": [
    "# class InteractionBlock(nn.Module):\n",
    "#     def __init__(self, embed_dim, ffn_dim, BertLayerNorm = ESM1bLayerNorm, attention_heads = 20, add_bias_kv = True, use_rotary_embeddings = True):\n",
    "#         super(InteractionBlock, self).__init__()\n",
    "#         # self.injector_query_norm = norm_layer(embedding_dim)\n",
    "#         # self.injector_kv_norm = norm_layer(embedding_dim)\n",
    "#         # self.extractor_query_norm = norm_layer(embedding_dim)\n",
    "#         # self.extractor_kv_norm = norm_layer(embedding_dim)\n",
    "#         # self.extractor_norm = norm_layer(embedding_dim)\n",
    "#         # self.injector = GroundingAttention(embedding_dim)\n",
    "#         # self.block = GroundingAttention(embedding_dim)\n",
    "#         # self.extractor = GroundingAttention(embedding_dim)\n",
    "#         # self.extractor_ffn = FFN(embedding_dim * ffn_dim, ffn_dim_rate * embedding_dim * ffn_dim)\n",
    "#         self.attention_heads = attention_heads\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.ffn_dim = ffn_dim\n",
    "#         self.injector_q_norm = BertLayerNorm(embed_dim)\n",
    "#         self.injector_kv_norm = BertLayerNorm(embed_dim)\n",
    "#         self.injector = GroundingAttention(embed_dim)\n",
    "#         self.block = TransformerLayer(\n",
    "#             self.embed_dim,\n",
    "#             4 * self.embed_dim, # embed_dim = 1280\n",
    "#             self.attention_heads, # 20\n",
    "#             add_bias_kv=False,\n",
    "#             use_esm1b_layer_norm=True,\n",
    "#             use_rotary_embeddings=True,\n",
    "#         )\n",
    "#         self.extractor_q_norm = BertLayerNorm(embed_dim)\n",
    "#         self.extractor_kv_norm = BertLayerNorm(embed_dim)\n",
    "#         self.extractor = GroundingAttention(embed_dim)\n",
    "#         self.ffn = FFN(embed_dim, ffn_dim)\n",
    "#     def forward(self, x, r, self_attn_mask=None, self_attn_padding_mask=None, need_head_weights=False):\n",
    "#         # x = self.injector(self.injector_query_norm(x), self.injector_kv_norm(r)) + x\n",
    "#         # x = self.block(x, x)\n",
    "#         # r = self.extractor(self.extractor_query_norm(r), self.extractor_kv_norm(x)) + r\n",
    "#         # r = r + self.extractor_ffn(self.extractor_norm(r))\n",
    "#         \n",
    "#         \n",
    "#         # x, _ = self.injector_attention(\n",
    "#         #     query=self.injector_q_norm(x),\n",
    "#         #     key=self.injector_kv_norm(r),\n",
    "#         #     value=self.injector_kv_norm(r),\n",
    "#         #     key_padding_mask=self_attn_padding_mask,\n",
    "#         #     need_weights=True,\n",
    "#         #     need_head_weights=need_head_weights,\n",
    "#         #     attn_mask=self_attn_mask,\n",
    "#         # )\n",
    "#         # print(self.injector_q_norm(x).shape)\n",
    "#         # print(self.injector_q_norm(r).shape)\n",
    "#         x = x + self.injector(self.injector_q_norm(x),self.injector_kv_norm(r))\n",
    "#         x, _ = self.block(x,\n",
    "#                           self_attn_padding_mask=self_attn_padding_mask,\n",
    "#                           need_head_weights=need_head_weights,\n",
    "#                           )\n",
    "#         r = r + self.extractor(self.extractor_q_norm(r),self.extractor_kv_norm(x))\n",
    "#         r = self.ffn(r)\n",
    "#         return x, r\n",
    "class InteractionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, ffn_dim, BertLayerNorm = ESM1bLayerNorm, attention_heads = 16, add_bias_kv = True, use_rotary_embeddings = True):\n",
    "        super(InteractionBlock, self).__init__()\n",
    "        # self.injector_query_norm = norm_layer(embedding_dim)\n",
    "        # self.injector_kv_norm = norm_layer(embedding_dim)\n",
    "        # self.extractor_query_norm = norm_layer(embedding_dim)\n",
    "        # self.extractor_kv_norm = norm_layer(embedding_dim)\n",
    "        # self.extractor_norm = norm_layer(embedding_dim)\n",
    "        # self.injector = GroundingAttention(embedding_dim)\n",
    "        # self.block = GroundingAttention(embedding_dim)\n",
    "        # self.extractor = GroundingAttention(embedding_dim)\n",
    "        # self.extractor_ffn = FFN(embedding_dim * ffn_dim, ffn_dim_rate * embedding_dim * ffn_dim)\n",
    "        self.attention_heads = attention_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.injector_q_norm = BertLayerNorm(embed_dim)\n",
    "        self.injector_kv_norm = BertLayerNorm(embed_dim)\n",
    "        # self.injector = GroundingAttention(embed_dim)\n",
    "\n",
    "        self.injector = torch.nn.MultiheadAttention(embed_dim = embed_dim, num_heads = attention_heads, dropout = 0.0)\n",
    "        self.block = bertlayer(embed_dim, embed_dim * 4)\n",
    "        self.extractor_q_norm = BertLayerNorm(embed_dim)\n",
    "        self.extractor_kv_norm = BertLayerNorm(embed_dim)\n",
    "        self.extractor = torch.nn.MultiheadAttention(embed_dim = embed_dim, num_heads = attention_heads, dropout = 0.0)\n",
    "        self.ffn = FFN(embed_dim, ffn_dim)\n",
    "    def forward(self, x, r, x_attn_padding_mask=None, r_attn_padding_mask = None, need_head_weights=False):\n",
    "        # x = self.injector(self.injector_query_norm(x), self.injector_kv_norm(r)) + x\n",
    "        # x = self.block(x, x)\n",
    "        # r = self.extractor(self.extractor_query_norm(r), self.extractor_kv_norm(x)) + r\n",
    "        # r = r + self.extractor_ffn(self.extractor_norm(r))\n",
    "        \n",
    "        \n",
    "        # x, _ = self.injector_attention(\n",
    "        #     query=self.injector_q_norm(x),\n",
    "        #     key=self.injector_kv_norm(r),\n",
    "        #     value=self.injector_kv_norm(r),\n",
    "        #     key_padding_mask=self_attn_padding_mask,\n",
    "        #     need_weights=True,\n",
    "        #     need_head_weights=need_head_weights,\n",
    "        #     attn_mask=self_attn_mask,\n",
    "        # )\n",
    "        # print(self.injector_q_norm(x).shape)\n",
    "        # print(self.injector_q_norm(r).shape)\n",
    "        \n",
    "        \n",
    "        # x = x + self.injector(self.injector_q_norm(x),self.injector_kv_norm(r))\n",
    "        residual_x = x\n",
    "        residual_r = r\n",
    "        # x = self.injector_q_norm(x)\n",
    "        # r = self.injector_kv_norm(r)\n",
    "        x = x.transpose(0, 1)\n",
    "        r = r.transpose(0, 1)\n",
    "        x = self.injector_q_norm(x)\n",
    "        r = self.injector_kv_norm(r)\n",
    "        # print(\"r.shape\")\n",
    "        # print(r.shape)\n",
    "        # print(\"r_attn_padding_mask.shape\")\n",
    "        # print(r_attn_padding_mask.shape)\n",
    "        # print(\"x.shape\")\n",
    "        # print(x.shape)\n",
    "        x, attn = self.injector(x, r, r, key_padding_mask=r_attn_padding_mask )\n",
    "        x = x.transpose(0, 1)\n",
    "        x = x + residual_x\n",
    "        r = residual_r\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.block(x, x_attn_padding_mask)\n",
    "        \n",
    "        # r = r + self.extractor(self.extractor_q_norm(r),self.extractor_kv_norm(x))\n",
    "        residual_r = r\n",
    "        residual_x = x.transpose(0, 1)\n",
    "        # x = self.extractor_kv_norm(x)\n",
    "        # r = self.extractor_q_norm(r)\n",
    "        # x = x.transpose(0, 1)\n",
    "        r = r.transpose(0, 1)\n",
    "        # print(r.shape)\n",
    "        # print(x.shape)\n",
    "        # print(x_attn_padding_mask.shape)\n",
    "        x = self.extractor_kv_norm(x)\n",
    "        r = self.extractor_q_norm(r)\n",
    "        r, attn = self.extractor(r, x, x, key_padding_mask=x_attn_padding_mask)\n",
    "        r = r.transpose(0, 1)\n",
    "        r = r + residual_r\n",
    "        \n",
    "        r = self.ffn(r)\n",
    "        return residual_x, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da4647de46f5d274",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:45:11.359130Z",
     "start_time": "2024-06-10T13:45:11.356459Z"
    }
   },
   "outputs": [],
   "source": [
    "# class part_test(nn.Module):\n",
    "#     def __init__(self, embedding_dim, ffn_dim, device, num_layers =36):\n",
    "#         super(part_test, self).__init__()\n",
    "#         self.layers = nn.ModuleList()\n",
    "#         for _ in range(num_layers):\n",
    "#             block = MultiheadAttention(\n",
    "#                     embedding_dim,\n",
    "#                     20,\n",
    "#                     add_bias_kv=False,\n",
    "#                     add_zero_attn=False,\n",
    "#                     use_rotary_embeddings=False,\n",
    "#                     encoder_decoder_attention=True\n",
    "#                 )\n",
    "#             self.layers.append(block)\n",
    "#     def forward(self, x, x_attn_padding_mask):\n",
    "#         for layer in self.layers:\n",
    "#             x, attn = layer(\n",
    "#                 query=x,\n",
    "#                 key=x,\n",
    "#                 value=x,\n",
    "#                 key_padding_mask=x_attn_padding_mask, #【[fffffttttt]】之类的\n",
    "#                 need_weights=False,\n",
    "#                 need_head_weights=False,# false\n",
    "#                 attn_mask=None, # None\n",
    "#             )\n",
    "#         return x\n",
    "# device = torch.device('npu:0')\n",
    "# modelb = part_test(1280, 1280  * 4,device)\n",
    "# X = torch.randn(30,15,1280)\n",
    "# x_attn_padding_mask = torch.zeros(15,30, dtype=torch.bool)\n",
    "# X = X.to(device)\n",
    "# x_attn_padding_mask = x_attn_padding_mask.to(device)\n",
    "# modelb.to(device)\n",
    "# opt = torch.optim.Adam(modelb.parameters(), lr=0.00001)\n",
    "# nuuu = 500000\n",
    "# for i in range(nuuu):\n",
    "#     x= modelb(X, x_attn_padding_mask)\n",
    "#     y = x.sum()\n",
    "#     y.backward()\n",
    "#     opt.step()\n",
    "#     # print(y)\n",
    "#     if i ==2:\n",
    "#         print(\"start\")\n",
    "#     if i ==nuuu - 1:\n",
    "#         print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea20878b-089b-4d1a-9efc-258ed8561984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_dim = 1280\n",
    "# modelb = TransformerLayer(\n",
    "#                 embedding_dim,\n",
    "#                 embedding_dim *4, # embed_dim = 1280\n",
    "#                 20, # 20\n",
    "#                 add_bias_kv=False,\n",
    "#                 use_esm1b_layer_norm=True,\n",
    "#                 use_rotary_embeddings=False,\n",
    "#             )\n",
    "# X = torch.randn(50,15,embedding_dim)\n",
    "# x_attn_padding_mask = torch.zeros(15,50, dtype=torch.bool)\n",
    "# X = X.to(device)\n",
    "# x_attn_padding_mask = x_attn_padding_mask.to(device)\n",
    "# modelb.to(device)\n",
    "# opt = torch_npu.optim.NpuFusedAdamW(modelb.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-2, amsgrad=False)\n",
    "# nuuu = 5000\n",
    "# for i in range(nuuu):\n",
    "#     x, _ = modelb(X,\n",
    "#         self_attn_padding_mask=x_attn_padding_mask,\n",
    "#         need_head_weights=False,\n",
    "#     )\n",
    "#     y = x.sum()\n",
    "#     y.backward()\n",
    "#     opt.step()\n",
    "#     # print(y)\n",
    "#     if i ==2:\n",
    "#         print(\"start\")\n",
    "#     if i ==nuuu - 1:\n",
    "#         print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be7d3a5fc1b548a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:45:11.411293Z",
     "start_time": "2024-06-10T13:45:11.403237Z"
    }
   },
   "outputs": [],
   "source": [
    "import esm\n",
    "class DynamicFeatureSelector(nn.Module):\n",
    "    def __init__(self, input_size, num_features, num_layers=4):\n",
    "        super(DynamicFeatureSelector, self).__init__()\n",
    "        \n",
    "        # 自动计算每层的节点数量\n",
    "        self.hidden_layers = []\n",
    "        self.batch_norm_layers = []  # 用于存储 BatchNorm 层\n",
    "        current_size = input_size\n",
    "        decrement = (input_size - num_features) // num_layers  # 递减的步长\n",
    "\n",
    "        # 添加隐藏层和 BatchNorm 层\n",
    "        for _ in range(num_layers):\n",
    "            if current_size <= num_features:\n",
    "                break\n",
    "            next_size = max(current_size - decrement, num_features)\n",
    "            self.hidden_layers.append(nn.Linear(current_size, next_size))\n",
    "            self.batch_norm_layers.append(nn.BatchNorm1d(next_size))  # 添加 BatchNorm 层\n",
    "            current_size = next_size\n",
    "\n",
    "        # 将隐藏层和 BatchNorm 层转换为 ModuleList\n",
    "        self.hidden_layers = nn.ModuleList(self.hidden_layers)\n",
    "        self.batch_norm_layers = nn.ModuleList(self.batch_norm_layers)\n",
    "        self.output_layer = nn.Linear(current_size, num_features)  # 输出层\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer, batch_norm in zip(self.hidden_layers, self.batch_norm_layers):\n",
    "            x = layer(x)  # 前向传播\n",
    "            x = batch_norm(x)  # 归一化\n",
    "            x = torch.relu(x)  # 激活函数\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "class mpi_adapter(nn.Module):\n",
    "    def __init__(self, embedding_dim, ffn_dim, num_layers =39):\n",
    "        super(mpi_adapter, self).__init__()\n",
    "        # self.embedding = nn.Embedding(num_embeddings=22, embedding_dim=embedding_dim)\n",
    "        # self.par_position = PositionalEncoding(num_hiddens=embedding_dim, max_len = par_len)\n",
    "        # self.mut_position = PositionalEncoding(num_hiddens=embedding_dim, max_len = mut_len)\n",
    "        # self.mut_esm, _ = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "        # _, self.alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "        # self.padding_idx = self.alphabet.padding_idx\n",
    "        # self.device = device\n",
    "        # self.esm.to(self.device)\n",
    "        # self.esm.eval()\n",
    "        self.liner_mut = nn.Linear(1280,embedding_dim)\n",
    "        self.liner_par = nn.Linear(1280,embedding_dim)\n",
    "        # self.batch_converter = self.alphabet.get_batch_converter()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(InteractionBlock(embedding_dim, ffn_dim))\n",
    "    def forward(self, mut0, mut1, par, mut0_padding_mask, par_padding_mask):\n",
    "        # print([(str(0), sequence) for sequence in mut0s])\n",
    "        # print([(str(0), sequence) for sequence in mut1s])\n",
    "        # print([(str(0), sequence) for sequence in pars])\n",
    "        # _, _, mut0 = self.batch_converter([(str(0), sequence) for sequence in mut0s])\n",
    "        # _, _, mut1 = self.batch_converter([(str(0), sequence) for sequence in mut1s])\n",
    "        # _, _, par = self.batch_converter([(str(0), sequence) for sequence in pars])\n",
    "        # print(par)\n",
    "        # mut0 = mut0s.to(self.device)\n",
    "        # mut1 = mut1s.to(self.device)\n",
    "        # par = pars.to(self.device)\n",
    "        # mut0_padding_mask = mut0_padding_mask.to(self.device)\n",
    "        # par_padding_mask = par_padding_mask.to(self.device)\n",
    "        # mut0_padding_mask = mut0.eq(self.padding_idx)\n",
    "        # mut1_padding_mask = mut1.eq(self.padding_idx)\n",
    "        # par_padding_mask = par.eq(self.padding_idx)\n",
    "        # mut0_padding_mask = torch.cat((mut0_padding_mask,mut1_padding_mask),dim=1)\n",
    "        mut0 = torch.cat((mut0,mut1),dim=1)\n",
    "        mut0 = self.liner_mut(mut0)\n",
    "        par = self.liner_par(par)\n",
    "        for layer in self.layers:\n",
    "            mut0, par = layer(mut0, par, x_attn_padding_mask = mut0_padding_mask, r_attn_padding_mask = par_padding_mask)\n",
    "        return par\n",
    "    \n",
    "# device = torch.device('npu:0')\n",
    "# # modelb =  TransformerLayer(\n",
    "# #             1280,\n",
    "# #             4000, # embed_dim = 1280\n",
    "# #             20, # 20\n",
    "# #             add_bias_kv=False,\n",
    "# #             use_esm1b_layer_norm=True,\n",
    "# #             use_rotary_embeddings=False,\n",
    "# #         )\n",
    "# modelb = mpi_adapter(1280*3, 1280 * 3 * 4,device)\n",
    "# X = torch.randn(10,15,1280)\n",
    "# X2 = torch.randn(10,15,1280)\n",
    "# R = torch.randn(10,100,1280)\n",
    "# x_attn_padding_mask = torch.zeros(10,30, dtype=torch.bool)\n",
    "# R_attn_padding_mask = torch.zeros(10,100, dtype=torch.bool)\n",
    "# X = X.to(device)\n",
    "# X2 = X2.to(device)\n",
    "# R = R.to(device)\n",
    "# x_attn_padding_mask = x_attn_padding_mask.to(device)\n",
    "# R_attn_padding_mask = R_attn_padding_mask.to(device)\n",
    "# modelb.to(device)\n",
    "# opt = torch.optim.Adam(modelb.parameters(), lr=0.00001)\n",
    "# nuuu = 500000\n",
    "# for i in range(nuuu):\n",
    "#     x= modelb(X, X2, R, x_attn_padding_mask, R_attn_padding_mask)\n",
    "#     y = x.sum()\n",
    "#     y.backward()\n",
    "#     opt.step()\n",
    "#     # print(y)\n",
    "#     if i ==2:\n",
    "#         print(\"start\")\n",
    "#     if i ==nuuu - 1:\n",
    "#         print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a948d9a7-84b9-429a-b655-bdecd488cf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in modelb.layers:\n",
    "#     print(next(layer.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d425e43-d985-4ed9-9b01-8b89035f88ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_dim = 1280*4\n",
    "# batch_size = 400\n",
    "# modelb = InteractionBlock(embedding_dim, embedding_dim * 4)\n",
    "# X = torch.randn(batch_size ,30,embedding_dim)\n",
    "# # X2 = torch.randn(batch_size ,15,embedding_dim)\n",
    "# R = torch.randn(batch_size ,100,embedding_dim)\n",
    "# x_attn_padding_mask = torch.zeros(batch_size ,30, dtype=torch.bool)\n",
    "# R_attn_padding_mask = torch.zeros(batch_size ,100, dtype=torch.bool)\n",
    "# X = X.to(device)\n",
    "# # X2 = X2.to(device)\n",
    "# R = R.to(device)\n",
    "# x_attn_padding_mask = x_attn_padding_mask.to(device)\n",
    "# R_attn_padding_mask = R_attn_padding_mask.to(device)\n",
    "# modelb.to(device)\n",
    "# opt = torch.optim.Adam(modelb.parameters(), lr=0.00001)\n",
    "# nuuu = 500000\n",
    "# for i in range(nuuu):\n",
    "#     x , _= modelb(X, R, x_attn_padding_mask, R_attn_padding_mask)\n",
    "#     y = x.sum()\n",
    "#     y.backward()\n",
    "#     opt.step()\n",
    "#     if i ==2:\n",
    "#         print(\"start\")\n",
    "#     if i ==nuuu - 1:\n",
    "#         print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54e341f1f47d3180",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:45:11.439194Z",
     "start_time": "2024-06-10T13:45:11.434447Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP_head(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_layers = 2):\n",
    "        super(MLP_head, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(GroundingAttention(embedding_dim))\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x)\n",
    "        return x[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd8d5415476c250c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:45:11.474135Z",
     "start_time": "2024-06-10T13:45:11.471531Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP_head_one_layer(nn.Module):\n",
    "    def __init__(self, embedding_dim, attention_heads=16):\n",
    "        super(MLP_head_one_layer, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=attention_heads, dropout=0.0)\n",
    "\n",
    "    def forward(self, x, x_attn_padding_mask, need_head_weights=False):\n",
    "        # 处理输入\n",
    "        result = x[:, 0:1, :]  # 取出第一位置的特征\n",
    "        x = x.transpose(0, 1)  # 转换为 (seq_length, batch_size, embedding_dim)\n",
    "        result = result.transpose(0, 1)  # 转换为 (batch_size, 1, embedding_dim)\n",
    "\n",
    "        # 多头自注意力计算\n",
    "        result, attn = self.attention(result, x, x, key_padding_mask=x_attn_padding_mask)\n",
    "\n",
    "        # 处理输出\n",
    "        result = result.transpose(0, 1)  # 转换回 (batch_size, 1, embedding_dim)\n",
    "        result = result.squeeze(1)  # 去掉大小为1的维度\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14f475c09191b81a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:45:11.513230Z",
     "start_time": "2024-06-10T13:45:11.506687Z"
    }
   },
   "outputs": [],
   "source": [
    "#bert中的带权交叉熵顺势函数\n",
    "class cross_entropy_bert(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(cross_entropy_bert, self).__init__()\n",
    "        # self.liner = nn.Linear(embedding_dim, 4)\n",
    "        self.activate = nn.Sigmoid()\n",
    "        self.device = device\n",
    "    def forward(self, b_labels, outputs, weights):\n",
    "        # weights = weights\n",
    "        # outputs = self.liner(outputs)\n",
    "        # print(\"output:\")\n",
    "        # print(outputs)\n",
    "        # outputs = self.activate(outputs)\n",
    "        labels = []\n",
    "        for index, fue in enumerate(b_labels):\n",
    "            labels.append(fue)\n",
    "        loss_sum = torch.tensor(0)\n",
    "        for i in range(outputs.shape[0]):\n",
    "            back_part = torch.tensor(0).to(device)\n",
    "            for j in outputs[i]:\n",
    "                back_part = back_part + torch.exp(j).to(device)\n",
    "            back_part = torch.log(back_part).to(device)\n",
    "            loss_sum = weights[labels[i]] * ((-1) * outputs[i][labels[i]] + back_part)\n",
    "        loss_sum = loss_sum/outputs.shape[0]\n",
    "        return loss_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5e8fd6a6884286d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:45:11.547641Z",
     "start_time": "2024-06-10T13:45:11.542467Z"
    }
   },
   "outputs": [],
   "source": [
    "# class Sum_model(nn.Module):\n",
    "#     def __init__(self, device, embedding_dim = 192,):\n",
    "#         super(Sum_model, self).__init__()\n",
    "#         self.device = device\n",
    "#         self.result = nn.Parameter(torch.randn(1280))\n",
    "#         self.backbone = mpi_adapter(embedding_dim, embedding_dim*4,device)\n",
    "#         self.neck = MLP_head_one_layer(embedding_dim)\n",
    "#         # self.head = cross_entropy_bert(embedding_dim, device)\n",
    "#         self.head = nn.Linear(embedding_dim, 4)\n",
    "#     def forward(self,  mut0s, mut1s, pars, mut0_padding_mask, par_padding_mask, weight, label):\n",
    "#         # mut0s = mut0s.to(self.device)\n",
    "#         # mut1s = mut1s.to(self.device)\n",
    "#         # pars = pars.to(self.device)\n",
    "#         # mut0_padding_mask = mut0_padding_mask.to(self.device)\n",
    "#         # par_padding_mask = par_padding_mask.to(self.device)\n",
    "\n",
    "\n",
    "#         res = self.result.repeat(pars.shape[0], 1).unsqueeze(1) \n",
    "#         pars = torch.concat([res,pars], dim = 1)\n",
    "#         false_column = torch.zeros(par_padding_mask.size(0), 1, dtype=torch.bool).to(self.device)\n",
    "#         # 在第一列前添加一排 False\n",
    "#         par_padding_mask = torch.cat((false_column, par_padding_mask), dim=1)\n",
    "\n",
    "\n",
    "#         x = self.backbone(mut0s, mut1s, pars, mut0_padding_mask, par_padding_mask)\n",
    "#         # print(\"x1:\")\n",
    "#         # x = self.neck(pars, par_padding_mask)\n",
    "#         x = self.neck(x, par_padding_mask)\n",
    "#         # # print(\"x2\")\n",
    "#         # # print(x)\n",
    "#         # # x = self.head(label, x, weight)\n",
    "#         x = self.head(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a2e733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Sum_model(nn.Module):\n",
    "    def __init__(self, embedding_dim=192):\n",
    "        super(Sum_model, self).__init__()\n",
    "        self.result = nn.Parameter(torch.randn(1280))  # 不再指定设备\n",
    "        self.backbone = mpi_adapter(embedding_dim, embedding_dim * 4)\n",
    "        self.neck = MLP_head_one_layer(embedding_dim)\n",
    "        self.head = nn.Linear(embedding_dim, 4)\n",
    "\n",
    "    def forward(self, mut0s, mut1s, pars, mut0_padding_mask, par_padding_mask, weight, label):\n",
    "        # 确保所有输入在同一设备上\n",
    "        mut0s = mut0s.to(pars.device)  # 使用 pars 的设备\n",
    "        mut1s = mut1s.to(pars.device)\n",
    "        pars = pars.to(pars.device)\n",
    "        mut0_padding_mask = mut0_padding_mask.to(pars.device)\n",
    "        par_padding_mask = par_padding_mask.to(pars.device)\n",
    "\n",
    "        # 扩展结果并拼接\n",
    "        res = self.result.repeat(pars.shape[0], 1).unsqueeze(1)  # (batch_size, 1, embedding_dim)\n",
    "        pars = torch.cat([res, pars], dim=1)  # 拼接操作\n",
    "\n",
    "        # 创建并拼接 padding mask\n",
    "        false_column = torch.zeros(par_padding_mask.size(0), 1, dtype=torch.bool).to(pars.device)\n",
    "        par_padding_mask = torch.cat((false_column, par_padding_mask), dim=1)  # 在第一列前添加一排 False\n",
    "\n",
    "        # 通过 backbone 和 neck 进行前向传播\n",
    "        x = self.backbone(mut0s, mut1s, pars, mut0_padding_mask, par_padding_mask)\n",
    "        x = self.neck(x, par_padding_mask)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3283086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda:0')\n",
    "# model = Sum_model()\n",
    "# model.to(device)\n",
    "# mut0 = torch.randn(7, 21, 1280).to(device)\n",
    "# mut1 = torch.randn(7, 21, 1280).to(device)\n",
    "# par0 = torch.randn(7, 593, 1280).to(device)\n",
    "# mut0_mask = torch.randint(0, 2, (7, 21), dtype=torch.bool).to(device) # 随机生成布尔值\n",
    "# mut1_mask = torch.randint(0, 2, (7, 21), dtype=torch.bool).to(device) # 随机生成布尔值\n",
    "# par0_mask = torch.randint(0, 2, (7, 593), dtype=torch.bool).to(device) # 随机生成布尔值\n",
    "# model(mut0, mut1, par0,torch.cat((mut0_mask,mut1_mask),dim=1),par0_mask, None,None).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca539eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sum_model()\n",
    "# model.to(device)\n",
    "# model = nn.DataParallel(model, device_ids=[0, 1, 2])  # 使用 DataParallel\n",
    "\n",
    "# num_epch =21 \n",
    "# mut0 = torch.randn(num_epch, 21, 1280).to(device)\n",
    "# mut1 = torch.randn(num_epch, 21, 1280).to(device)\n",
    "# par0 = torch.randn(num_epch, 1000, 1280).to(device)\n",
    "# mut0_mask = torch.randint(0, 2, (num_epch, 21), dtype=torch.bool).to(device) # 随机生成布尔值\n",
    "# mut1_mask = torch.randint(0, 2, (num_epch, 21), dtype=torch.bool).to(device) # 随机生成布尔值\n",
    "# par0_mask = torch.randint(0, 2, (num_epch, 1000), dtype=torch.bool).to(device) # 随机生成布尔值\n",
    "# # 定义损失和优化器\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = nn.MSELoss()  # 示例损失函数\n",
    "\n",
    "# # 模拟训练循环\n",
    "# for epoch in range(100000):\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     # 前向传播\n",
    "#     # output = model(x)\n",
    "#     mut = model(mut0, mut1, par0 ,torch.cat((mut0_mask, mut1_mask),dim = 1) ,par0_mask, None, None)\n",
    "#     # 计算损失\n",
    "#     loss = mut.mean()\n",
    "\n",
    "#     # 反向传播\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d516c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # model = MultiHeadAttentionModel(embed_dim, num_heads)\n",
    "# model = mpi_adapter(1280, 1280 *4, device)\n",
    "# model = nn.DataParallel(model, device_ids=[0, 1, 2])  # 使用 DataParallel\n",
    "# model.to(device)\n",
    "# num_epch =21 \n",
    "# mut0 = torch.randn(num_epch, 21, 1280).to(device)\n",
    "# mut1 = torch.randn(num_epch, 21, 1280).to(device)\n",
    "# par0 = torch.randn(num_epch, 1000, 1280).to(device)\n",
    "# mut0_mask = torch.randint(0, 2, (num_epch, 21), dtype=torch.bool).to(device) # 随机生成布尔值\n",
    "# mut1_mask = torch.randint(0, 2, (num_epch, 21), dtype=torch.bool).to(device) # 随机生成布尔值\n",
    "# par0_mask = torch.randint(0, 2, (num_epch, 1000), dtype=torch.bool).to(device) # 随机生成布尔值\n",
    "# # 定义损失和优化器\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = nn.MSELoss()  # 示例损失函数\n",
    "\n",
    "# # 模拟训练循环\n",
    "# for epoch in range(100000):\n",
    "#     model.train()\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     # 前向传播\n",
    "#     # output = model(x)\n",
    "#     mut = model(mut0, mut1, par0 ,torch.cat((mut0_mask, mut1_mask),dim = 1) ,par0_mask)\n",
    "#     # 计算损失\n",
    "#     loss = mut.mean()\n",
    "\n",
    "#     # 反向传播\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aeb5a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # 定义设备和模型\n",
    "# device = torch.device('cuda:0')\n",
    "# model = Sum_model(device)\n",
    "# model = torch.nn.DataParallel(model, device_ids=[0, 1, 2])  # 使用 DataParallel 在多张卡上训练\n",
    "# model.to(device)\n",
    "\n",
    "# # 生成随机输入数据\n",
    "# mut0 = torch.randn(7, 21, 1280).to(device)\n",
    "# mut1 = torch.randn(7, 21, 1280).to(device)\n",
    "# par0 = torch.randn(7, 593, 1280).to(device)\n",
    "# mut0_mask = torch.randint(0, 2, (7, 21), dtype=torch.bool).to(device)  # 随机生成布尔值\n",
    "# mut1_mask = torch.randint(0, 2, (7, 21), dtype=torch.bool).to(device)  # 随机生成布尔值\n",
    "# par0_mask = torch.randint(0, 2, (7, 593), dtype=torch.bool).to(device)  # 随机生成布尔值\n",
    "\n",
    "# # 在模型前向传播时使用 DataParallel\n",
    "# output_shape = model(mut0, mut1, par0, torch.cat((mut0_mask, mut1_mask), dim=1), par0_mask, None, None).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4184a5a3bdf4eaef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:45:11.592148Z",
     "start_time": "2024-06-10T13:45:11.586592Z"
    }
   },
   "outputs": [],
   "source": [
    "class PandasDataReader:\n",
    "    def __init__(self, df, batch_size=1, shuffle=False):\n",
    "        self.df = df.sample(frac=1).reset_index(drop=True) if shuffle else df\n",
    "        self.batch_size = batch_size\n",
    "        self.current_index = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_index >= len(self.df):\n",
    "            raise StopIteration()\n",
    "\n",
    "        batch = self.df.iloc[self.current_index:self.current_index+self.batch_size]\n",
    "        self.current_index += self.batch_size\n",
    "\n",
    "        # 如果 batch_size 为 1, 则直接返回 batch 的第一行\n",
    "        return batch.iloc[0] if self.batch_size == 1 else batch\n",
    "# data_reader = PandasDataReader(df, batch_size=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a82882c-9e80-4015-8b7b-f86dba99773d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:45:11.633730Z",
     "start_time": "2024-06-10T13:45:11.625740Z"
    }
   },
   "outputs": [],
   "source": [
    "class ESMModelWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ESMModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, batch_tokens, repr_layers=[33], return_contacts=False):\n",
    "        return self.model(batch_tokens, repr_layers=repr_layers, return_contacts=return_contacts)\n",
    "\n",
    "\n",
    "class ESMFeatureEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ESMFeatureEncoder, self).__init__()\n",
    "        self.device = 'cuda:2'\n",
    "        self.model, self.alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "        self.model.to(self.device)\n",
    "        # self.model = torch.nn.DataParallel(self.model, device_ids=[0, 1, 2])\n",
    "        self.model.eval()  # Set the model to evaluation mode\n",
    "        self.batch_converter = self.alphabet.get_batch_converter()\n",
    "        self.padding_idx = self.alphabet.padding_idx\n",
    "        # Wrap the model with the ESMModelWrapper\n",
    "        self.model = ESMModelWrapper(self.model)\n",
    "\n",
    "    def encode(self, sequences):\n",
    "        batch_labels, batch_strs, batch_tokens = self.batch_converter([(str(0), sequence) for sequence in sequences])\n",
    "        batch_tokens = batch_tokens.to(self.device)\n",
    "        batch_mask = batch_tokens.eq(self.padding_idx)\n",
    "        # print(batch_tokens.shape)\n",
    "        with torch.no_grad():\n",
    "            results = self.model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "        token_representations = results['representations'][33]\n",
    "        # print(results['representations'][33].mean(dim=1).unsqueeze(1).shape)\n",
    "        return token_representations,batch_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fdf89b-5dff-4b16-a83c-c48b526df38d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0078ee5039c96a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:45:12.184538Z",
     "start_time": "2024-06-10T13:45:11.673111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37639, 26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#Feature AC</th>\n",
       "      <th>Feature range(s)</th>\n",
       "      <th>Original sequence</th>\n",
       "      <th>Resulting sequence</th>\n",
       "      <th>Feature short label</th>\n",
       "      <th>Feature type</th>\n",
       "      <th>Feature annotation</th>\n",
       "      <th>Affected protein AC</th>\n",
       "      <th>Affected protein symbol</th>\n",
       "      <th>Affected protein full name</th>\n",
       "      <th>...</th>\n",
       "      <th>n_partner</th>\n",
       "      <th>mutAC</th>\n",
       "      <th>mut0</th>\n",
       "      <th>parAC</th>\n",
       "      <th>par0</th>\n",
       "      <th>mut1</th>\n",
       "      <th>label</th>\n",
       "      <th>mutAC1</th>\n",
       "      <th>position_total</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EBI-10039489</td>\n",
       "      <td>[81-81]</td>\n",
       "      <td>[V]</td>\n",
       "      <td>[E]</td>\n",
       "      <td>p.Val81Glu</td>\n",
       "      <td>mutation disrupting(MI:0573)</td>\n",
       "      <td></td>\n",
       "      <td>P28795</td>\n",
       "      <td>PEX3</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>P28795</td>\n",
       "      <td>MAPNQRSRSLLQRHRGKVLISLTGIAALFTTGSVVVFFVKRWLYKQ...</td>\n",
       "      <td>Q03694</td>\n",
       "      <td>MVLSRGETKKNSVRLTAKQEKKPQSTFQTLKQSLKLSNNKKLKQDS...</td>\n",
       "      <td>MAPNQRSRSLLQRHRGKVLISLTGIAALFTTGSVVVFFVKRWLYKQ...</td>\n",
       "      <td>0</td>\n",
       "      <td>P28795_Val81Glu</td>\n",
       "      <td>[81]</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EBI-10039495</td>\n",
       "      <td>[188-188]</td>\n",
       "      <td>[N]</td>\n",
       "      <td>[I]</td>\n",
       "      <td>p.Asn188Ile</td>\n",
       "      <td>mutation decreasing(MI:0119)</td>\n",
       "      <td></td>\n",
       "      <td>P28795</td>\n",
       "      <td>PEX3</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>P28795</td>\n",
       "      <td>MAPNQRSRSLLQRHRGKVLISLTGIAALFTTGSVVVFFVKRWLYKQ...</td>\n",
       "      <td>Q03694</td>\n",
       "      <td>MVLSRGETKKNSVRLTAKQEKKPQSTFQTLKQSLKLSNNKKLKQDS...</td>\n",
       "      <td>MAPNQRSRSLLQRHRGKVLISLTGIAALFTTGSVVVFFVKRWLYKQ...</td>\n",
       "      <td>1</td>\n",
       "      <td>P28795_Asn188Ile</td>\n",
       "      <td>[188]</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EBI-10039551</td>\n",
       "      <td>[81-81]</td>\n",
       "      <td>[V]</td>\n",
       "      <td>[E]</td>\n",
       "      <td>p.Val81Glu</td>\n",
       "      <td>mutation disrupting(MI:0573)</td>\n",
       "      <td></td>\n",
       "      <td>P28795</td>\n",
       "      <td>PEX3</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>P28795</td>\n",
       "      <td>MAPNQRSRSLLQRHRGKVLISLTGIAALFTTGSVVVFFVKRWLYKQ...</td>\n",
       "      <td>Q03694</td>\n",
       "      <td>MVLSRGETKKNSVRLTAKQEKKPQSTFQTLKQSLKLSNNKKLKQDS...</td>\n",
       "      <td>MAPNQRSRSLLQRHRGKVLISLTGIAALFTTGSVVVFFVKRWLYKQ...</td>\n",
       "      <td>0</td>\n",
       "      <td>P28795_Val81Glu</td>\n",
       "      <td>[81]</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EBI-10039706</td>\n",
       "      <td>[81-81]</td>\n",
       "      <td>[V]</td>\n",
       "      <td>[E]</td>\n",
       "      <td>p.Val81Glu</td>\n",
       "      <td>mutation disrupting(MI:0573)</td>\n",
       "      <td></td>\n",
       "      <td>P28795</td>\n",
       "      <td>PEX3</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>P28795</td>\n",
       "      <td>MAPNQRSRSLLQRHRGKVLISLTGIAALFTTGSVVVFFVKRWLYKQ...</td>\n",
       "      <td>Q03694</td>\n",
       "      <td>MVLSRGETKKNSVRLTAKQEKKPQSTFQTLKQSLKLSNNKKLKQDS...</td>\n",
       "      <td>MAPNQRSRSLLQRHRGKVLISLTGIAALFTTGSVVVFFVKRWLYKQ...</td>\n",
       "      <td>0</td>\n",
       "      <td>P28795_Val81Glu</td>\n",
       "      <td>[81]</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EBI-10039722</td>\n",
       "      <td>[81-81]</td>\n",
       "      <td>[V]</td>\n",
       "      <td>[E]</td>\n",
       "      <td>p.Val81Glu</td>\n",
       "      <td>mutation disrupting(MI:0573)</td>\n",
       "      <td></td>\n",
       "      <td>P28795</td>\n",
       "      <td>PEX3</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>P28795</td>\n",
       "      <td>MAPNQRSRSLLQRHRGKVLISLTGIAALFTTGSVVVFFVKRWLYKQ...</td>\n",
       "      <td>Q03694</td>\n",
       "      <td>MVLSRGETKKNSVRLTAKQEKKPQSTFQTLKQSLKLSNNKKLKQDS...</td>\n",
       "      <td>MAPNQRSRSLLQRHRGKVLISLTGIAALFTTGSVVVFFVKRWLYKQ...</td>\n",
       "      <td>0</td>\n",
       "      <td>P28795_Val81Glu</td>\n",
       "      <td>[81]</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    #Feature AC Feature range(s) Original sequence Resulting sequence  \\\n",
       "0  EBI-10039489          [81-81]               [V]                [E]   \n",
       "1  EBI-10039495        [188-188]               [N]                [I]   \n",
       "2  EBI-10039551          [81-81]               [V]                [E]   \n",
       "3  EBI-10039706          [81-81]               [V]                [E]   \n",
       "4  EBI-10039722          [81-81]               [V]                [E]   \n",
       "\n",
       "  Feature short label                  Feature type Feature annotation  \\\n",
       "0          p.Val81Glu  mutation disrupting(MI:0573)                      \n",
       "1         p.Asn188Ile  mutation decreasing(MI:0119)                      \n",
       "2          p.Val81Glu  mutation disrupting(MI:0573)                      \n",
       "3          p.Val81Glu  mutation disrupting(MI:0573)                      \n",
       "4          p.Val81Glu  mutation disrupting(MI:0573)                      \n",
       "\n",
       "  Affected protein AC Affected protein symbol Affected protein full name  ...  \\\n",
       "0              P28795                    PEX3                             ...   \n",
       "1              P28795                    PEX3                             ...   \n",
       "2              P28795                    PEX3                             ...   \n",
       "3              P28795                    PEX3                             ...   \n",
       "4              P28795                    PEX3                             ...   \n",
       "\n",
       "  n_partner   mutAC                                               mut0  \\\n",
       "0         2  P28795  MAPNQRSRSLLQRHRGKVLISLTGIAALFTTGSVVVFFVKRWLYKQ...   \n",
       "1         2  P28795  MAPNQRSRSLLQRHRGKVLISLTGIAALFTTGSVVVFFVKRWLYKQ...   \n",
       "2         2  P28795  MAPNQRSRSLLQRHRGKVLISLTGIAALFTTGSVVVFFVKRWLYKQ...   \n",
       "3         2  P28795  MAPNQRSRSLLQRHRGKVLISLTGIAALFTTGSVVVFFVKRWLYKQ...   \n",
       "4         2  P28795  MAPNQRSRSLLQRHRGKVLISLTGIAALFTTGSVVVFFVKRWLYKQ...   \n",
       "\n",
       "    parAC                                               par0  \\\n",
       "0  Q03694  MVLSRGETKKNSVRLTAKQEKKPQSTFQTLKQSLKLSNNKKLKQDS...   \n",
       "1  Q03694  MVLSRGETKKNSVRLTAKQEKKPQSTFQTLKQSLKLSNNKKLKQDS...   \n",
       "2  Q03694  MVLSRGETKKNSVRLTAKQEKKPQSTFQTLKQSLKLSNNKKLKQDS...   \n",
       "3  Q03694  MVLSRGETKKNSVRLTAKQEKKPQSTFQTLKQSLKLSNNKKLKQDS...   \n",
       "4  Q03694  MVLSRGETKKNSVRLTAKQEKKPQSTFQTLKQSLKLSNNKKLKQDS...   \n",
       "\n",
       "                                                mut1  label            mutAC1  \\\n",
       "0  MAPNQRSRSLLQRHRGKVLISLTGIAALFTTGSVVVFFVKRWLYKQ...      0   P28795_Val81Glu   \n",
       "1  MAPNQRSRSLLQRHRGKVLISLTGIAALFTTGSVVVFFVKRWLYKQ...      1  P28795_Asn188Ile   \n",
       "2  MAPNQRSRSLLQRHRGKVLISLTGIAALFTTGSVVVFFVKRWLYKQ...      0   P28795_Val81Glu   \n",
       "3  MAPNQRSRSLLQRHRGKVLISLTGIAALFTTGSVVVFFVKRWLYKQ...      0   P28795_Val81Glu   \n",
       "4  MAPNQRSRSLLQRHRGKVLISLTGIAALFTTGSVVVFFVKRWLYKQ...      0   P28795_Val81Glu   \n",
       "\n",
       "  position_total position  \n",
       "0           [81]       81  \n",
       "1          [188]      188  \n",
       "2           [81]       81  \n",
       "3           [81]       81  \n",
       "4           [81]       81  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_path = 'mippi/processed_mutations.dataset'\n",
    "df = pd.read_pickle(data_path)\n",
    "\n",
    "new_df1 = df[df['label'] == 1].copy()\n",
    "new_df1['mut0'], new_df1['mut1'] = new_df1['mut1'], new_df1['mut0']\n",
    "new_df1['label'] = 3\n",
    "new_df2 = df[df['label'] == 3].copy()\n",
    "new_df2['mut0'], new_df2['mut1'] = new_df2['mut1'], new_df2['mut0']\n",
    "new_df2['label'] = 1\n",
    "\n",
    "new_df3 = df[df['label'] == 2].copy()\n",
    "new_df3['mut0'], new_df3['mut1'] = new_df3['mut1'], new_df3['mut0']\n",
    "new_df3['label'] = 2\n",
    "df = pd.concat([df, new_df1, new_df2, new_df3], ignore_index=True)\n",
    "df[\"position_total\"] = df[\"Feature range(s)\"].apply(\n",
    "    lambda x: sorted(set([int(y.split(\"-\")[0]) for y in x] + [int(y.split(\"-\")[1]) for y in x]))\n",
    ")\n",
    "df = df[df['mut0'] != df['mut1']]\n",
    "df[\"position\"] = df[\"position_total\"].apply(lambda x: math.ceil((min(x) + max(x)) / 2))\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea2e4a2fc82b6203",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:45:12.227042Z",
     "start_time": "2024-06-10T13:45:12.186618Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36257, 26)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_half_protein_used =10 \n",
    "# seq_lengths = df['Original sequence'].apply(lambda x: len(x[0]))\n",
    "# # 选择唯一元素长度小于等于 10 的行\n",
    "# df_filtered = df.loc[seq_lengths <= len_half_protein_used]\n",
    "# seq_lengths = df['Resulting sequence'].apply(lambda x: len(x[0]))\n",
    "# # 选择唯一元素长度小于等于 10 的行\n",
    "# df_filtered = df.loc[seq_lengths <= len_half_protein_used]\n",
    "df = df[df[\"position_total\"].apply(lambda x: max(x) - min(x) < 2* len_half_protein_used)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbeee234e73fb2e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:45:12.249871Z",
     "start_time": "2024-06-10T13:45:12.228919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30173, 26)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['mut0'].str.len() <= 1500]\n",
    "df = df[df['par0'].str.len() <= 1000]\n",
    "df = df[df['label'] != 4]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9640a559-cd38-4338-9160-c0d1779f2e71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T13:45:12.255684Z",
     "start_time": "2024-06-10T13:45:12.252646Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class GHMC_Loss(nn.Module):\n",
    "    def __init__(self, device, bins=8, momentum=0.3):\n",
    "        super(GHMC_Loss, self).__init__()\n",
    "        self.device = device\n",
    "        self.bins = bins\n",
    "        self.momentum = momentum\n",
    "        self.edges = [float(x) / self.bins for x in range(self.bins + 1)]\n",
    "        if momentum > 0:\n",
    "            self.acc_sum = np.zeros(bins)\n",
    "\n",
    "    def forward(self, targets, logits, no_meaning):\n",
    "        targets = torch.tensor(targets.to_list())\n",
    "        targets = F.one_hot(targets, num_classes=4).float().to(self.device)\n",
    "        # Calculate gradient norm\n",
    "        edges = self.edges\n",
    "        mmt = self.momentum\n",
    "        weights = torch.zeros_like(logits)\n",
    "        g = torch.abs(logits.softmax(dim=1).detach().to(self.device) - targets.to(self.device))\n",
    "\n",
    "        tot = logits.shape[0] * logits.shape[1]  # Total number of elements\n",
    "        n = 0  # n valid bins\n",
    "        for i in range(self.bins):\n",
    "            inds = (g >= edges[i]) & (g < edges[i + 1])\n",
    "            num_in_bin = inds.sum().item()\n",
    "            if num_in_bin > 0:\n",
    "                if mmt > 0:\n",
    "                    self.acc_sum[i] = mmt * self.acc_sum[i] + (1 - mmt) * num_in_bin\n",
    "                    weights[inds] = tot / self.acc_sum[i]\n",
    "                else:\n",
    "                    weights[inds] = tot / num_in_bin\n",
    "                n += 1\n",
    "        if n > 0:\n",
    "            weights = weights / n\n",
    "\n",
    "        # Flatten targets to match logits shape\n",
    "        targets = targets.argmax(dim=1)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.cross_entropy(logits, targets, reduction='none')\n",
    "\n",
    "        # Apply weights to the loss\n",
    "        weights = weights.max(dim=1)[0]  # Get the maximum weight for each sample\n",
    "        loss = (loss * weights).sum() / tot\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e4c624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(max_len, d_model):\n",
    "    # 创建位置编码矩阵\n",
    "    position = torch.arange(max_len, dtype=torch.float).unsqueeze(1)  # 形状为 (max_len, 1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))  # 计算分母\n",
    "    \n",
    "    # 计算位置编码\n",
    "    pe = torch.zeros(max_len, d_model)  # 初始化位置编码矩阵\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)  # 偶数维度\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)  # 奇数维度\n",
    "    pe.requires_grad = False\n",
    "    return pe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f49130255e036e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-10T13:45:12.257644Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [23331/24138], Loss: 0.491074800491333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json\n",
    "batch_size = 4\n",
    "# 五折交叉验证\n",
    "splits = 5\n",
    "# Create StratifiedKFold object.\n",
    "# 创建StratifiedKFold对象    StratifiedKFold是sklearn库中的一个类，用于将数据集进行分层抽样设置了4个分割（n_splits=splits），打乱数据顺序（shuffle=True）并设置随机种子（random_state=1）。\n",
    "skf = StratifiedKFold(n_splits=splits, shuffle=True)# , random_state=1\n",
    "# device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device('cuda:2')\n",
    "# 定义优化器\n",
    "model = Sum_model()\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "# model = torch.nn.DataParallel(model, device_ids=[0, 2])\n",
    "model_loss = GHMC_Loss(device)\n",
    "esm_model = ESMFeatureEncoder()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, betas=(0.9, 0.999), weight_decay=1e-6, amsgrad=False)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=6e-5)\n",
    "for train_index, test_index in skf.split(df,df[\"label\"]):#它接受两个参数：x 和 y，分别表示特征数据和目标数据\n",
    "    x_train_fold, x_test_fold = df.iloc[train_index], df.iloc[test_index]#iloc是pandas库中的一个属性，用于基于整数位置的索引\n",
    "    break\n",
    "    # y_train_fold, y_test_fold = df[\"label\"].iloc[train_index], df[\"label\"].iloc[test_index]\n",
    "x_train_fold.to_csv('data/x_train_fold_mirror_multi.csv')\n",
    "x_test_fold.to_csv('data/x_test_fold_mirror_multi.csv')\n",
    "class_weights = compute_class_weight(class_weight='balanced',classes=np.unique(x_train_fold['label']),y=np.ravel(x_train_fold[\"label\"]))\n",
    "# print(class_weights)\n",
    "# class_weights_list.append(class_weights)\n",
    "batch_size = 7\n",
    "optimizer.zero_grad()\n",
    "loss_range = []\n",
    "acc_test = []\n",
    "acc_train = []\n",
    "position_embedding = positional_encoding(4 * len_half_protein_used + 2 , 1280).to(device)\n",
    "for epoc in range(40):\n",
    "    label_need_train = []\n",
    "    label_pred_train = []\n",
    "    label_need = []\n",
    "    label_pred = []\n",
    "    i = 0\n",
    "    loss_onebatch = 0\n",
    "    x_train_fold = x_train_fold.sample(frac=1).reset_index(drop=True)\n",
    "    data_reader = PandasDataReader(x_train_fold, batch_size=batch_size, shuffle=True)\n",
    "    num_epochs = len(x_train_fold)\n",
    "    for batch in data_reader:\n",
    "        if False:\n",
    "            loss = model([batch[\"mut0\"]],[ batch[\"mut1\"]], [batch[\"par0\"]], [max(int(batch[\"Feature range(s)\"][0].split(\"-\")[0])-25,0),max(int(batch[\"Feature range(s)\"][0].split(\"-\")[0])-25,0)+50],class_weights,[batch[\"label\"]],device) # 三个序列和需要的突变采样位置，如突变从80号开始就是[80-25，80+25]\n",
    "            loss_onebatch = loss_onebatch + loss.item()\n",
    "        else:\n",
    "            positions = batch[\"position\"].tolist()\n",
    "            # print([sequence for sequence in batch[\"mut0\"]][0])\n",
    "            mut0,mut0_mask = esm_model.encode([sequence for sequence in batch[\"mut0\"]])\n",
    "            mut1,mut1_mask = esm_model.encode([sequence for sequence in batch[\"mut1\"]])\n",
    "            par0,par0_mask = esm_model.encode([sequence for sequence in batch[\"par0\"]])\n",
    "            # #类似位置编码，标记突变前后的\n",
    "            # zero_row = torch.zeros(mut0.shape[0], mut0.shape[1], 1).to(device)\n",
    "            # mut0 = torch.cat((mut0, zero_row), dim=-1)\n",
    "            # zero_row = torch.ones(mut1.shape[0], mut1.shape[1], 1).to(device)\n",
    "            # mut1 = torch.cat((mut1, zero_row), dim=-1)\n",
    "            #处理mut0，mut1，去除0，找到位置，接入全局变量，01拼接在模型里\n",
    "            # mut0 = mut0[:, 1:, :]\n",
    "            if mut0.shape[1] < 2*len_half_protein_used:\n",
    "                mut0 = torch.cat((mut0.mean(dim=1, keepdim=True),mut0),dim=1)\n",
    "                mut0_mask = mut0_mask\n",
    "                mut1 = torch.cat((mut1.mean(dim=1, keepdim=True),mut1),dim=1)\n",
    "                mut1_mask = mut1_mask\n",
    "            else:\n",
    "                result = torch.randn(mut0.shape[0],2*len_half_protein_used,mut0.shape[2])\n",
    "                result_padding = (np.random.randint(0, 2, size=(mut0.shape[0], 2*len_half_protein_used)) == 1)\n",
    "\n",
    "                for ia in range(len(positions)):\n",
    "                    position = int(positions[ia])\n",
    "                    if position - len_half_protein_used < 0 :\n",
    "                        result[ia, :, :] = mut0[ia,:2 * len_half_protein_used,:]\n",
    "                        result_padding[ia, :] = mut0_mask[ia,:2 * len_half_protein_used].cpu()\n",
    "                    elif position + len_half_protein_used > mut0.shape[1] :\n",
    "                        result[ia, :, :] = mut0[ia,-2 * len_half_protein_used:,:]\n",
    "                        result_padding[ia, :] = mut0_mask[ia, -2 * len_half_protein_used].cpu()\n",
    "                    else:\n",
    "                        result[ia, :, :] = mut0[ia,position - len_half_protein_used:position + len_half_protein_used,:]\n",
    "                        result_padding[ia, :] = mut0_mask[ia, position - len_half_protein_used : position + len_half_protein_used].cpu()\n",
    "\n",
    "                mut0 = torch.cat((mut0.mean(dim=1, keepdim=True).cpu(), result),dim=1)\n",
    "                mut0_mask = result_padding\n",
    "                mut0_mask = np.concatenate([np.full((mut0_mask.shape[0], 1), False), mut0_mask], axis=1)\n",
    "                \n",
    "                \n",
    "                \n",
    "                result = torch.randn(mut1.shape[0],2*len_half_protein_used,mut1.shape[2])\n",
    "                result_padding = (np.random.randint(0, 2, size=(mut1.shape[0], 2*len_half_protein_used)) == 1)\n",
    "                for ia in range(len(positions)):\n",
    "                    position = int(positions[ia])\n",
    "                    if position - len_half_protein_used < 0 :\n",
    "                        result[ia, :, :] = mut1[ia,:2 * len_half_protein_used,:]\n",
    "                        result_padding[ia, :] = mut1_mask[ia,:2 * len_half_protein_used].cpu()\n",
    "                    elif position + len_half_protein_used > mut1.shape[1] :\n",
    "                        result[ia, :, :] = mut1[ia,-2 * len_half_protein_used : ,:]\n",
    "                        result_padding[ia, :] = mut1_mask[ia, -2 * len_half_protein_used].cpu()\n",
    "                    else:\n",
    "                        # print(i)\n",
    "                        result[ia, :, :] = mut1[ia,position - len_half_protein_used:position + len_half_protein_used,:]\n",
    "                        result_padding[ia, :] = mut1_mask[ia, position - len_half_protein_used: position + len_half_protein_used].cpu()\n",
    "                mut1 = torch.cat((mut1.mean(dim=1, keepdim=True).cpu(), result),dim=1)\n",
    "                mut1_mask = result_padding\n",
    "                mut1_mask = np.concatenate([np.full((mut1_mask.shape[0], 1), False), mut1_mask], axis=1)\n",
    "                \n",
    "                \n",
    "            mut0_mask = torch.from_numpy(mut0_mask)\n",
    "            mut0_mask = mut0_mask.to(device)\n",
    "            mut1_mask = torch.from_numpy(mut1_mask)\n",
    "            mut1_mask = mut1_mask.to(device)\n",
    "            mut0 = mut0.to(device)\n",
    "            mut1 = mut1.to(device)\n",
    "            mut0 = mut0 + position_embedding[:mut0.shape[1] , : ]\n",
    "            mut1 = mut1 + position_embedding[2 * len_half_protein_used + 1 : 2 * len_half_protein_used + 1 + mut1.shape[1] , : ]\n",
    "            par0 = par0.to(device)\n",
    "            # par0_mask = torch.from_numpy(par0_mask)\n",
    "            # for position in batch[\"Feature range(s)\"]:\n",
    "            #     positions.append([max(int(position[0].split(\"-\")[0])-25,0),max(int(position[0].split(\"-\")[0])-25,0)+50])\n",
    "            out = model(mut0, mut1, par0,torch.cat((mut0_mask,mut1_mask),dim=1),par0_mask, class_weights,batch[\"label\"])\n",
    "            loss = model_loss(batch[\"label\"], out,class_weights)\n",
    "            _, predicted_labels = torch.max(out, 1)\n",
    "            # break\n",
    "            # loss = ghm_loss(out, batch[\"label\"].to_list())\n",
    "            # loss_onebatch = loss_onebatch + loss.item()\n",
    "            loss_range.append(loss.item())\n",
    "            # print(loss.item())\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(\"done\")\n",
    "        label_need_train = label_need_train + batch[\"label\"].to_list()\n",
    "        label_pred_train = label_pred_train + predicted_labels.tolist()\n",
    "        if (i) % batch_size*20 == 0:\n",
    "            clear_output()\n",
    "            print(f' [{i}/{num_epochs}], Loss: {loss}')\n",
    "\n",
    "        i = i+batch_size\n",
    "    accuracy = accuracy_score(label_need_train,label_pred_train)\n",
    "    acc_train.append(accuracy)\n",
    "    torch.save(model.state_dict(), 'model_params_mirror_multi.pth')\n",
    "    model.eval()\n",
    "    data_reader = PandasDataReader(x_test_fold, batch_size=10, shuffle=True)\n",
    "    num_epochs = len(x_train_fold)\n",
    "    print(\"testing\",epoc)\n",
    "    for batch in data_reader:\n",
    "        if False:\n",
    "            loss = model([batch[\"mut0\"]],[ batch[\"mut1\"]], [batch[\"par0\"]], [max(int(batch[\"Feature range(s)\"][0].split(\"-\")[0])-25,0),max(int(batch[\"Feature range(s)\"][0].split(\"-\")[0])-25,0)+50],class_weights,[batch[\"label\"]],device) # 三个序列和需要的突变采样位置，如突变从80号开始就是[80-25，80+25]\n",
    "            loss_onebatch = loss_onebatch + loss.item()\n",
    "        else:\n",
    "            positions = batch[\"position\"].tolist()\n",
    "            # print([sequence for sequence in batch[\"mut0\"]][0])\n",
    "            mut0,mut0_mask = esm_model.encode([sequence for sequence in batch[\"mut0\"]])\n",
    "            mut1,mut1_mask = esm_model.encode([sequence for sequence in batch[\"mut1\"]])\n",
    "            par0,par0_mask = esm_model.encode([sequence for sequence in batch[\"par0\"]])\n",
    "            # zero_row = torch.zeros(mut0.shape[0], mut0.shape[1], 1).to(device)\n",
    "            # mut0 = torch.cat((mut0, zero_row), dim=-1)\n",
    "            # zero_row = torch.ones(mut1.shape[0], mut1.shape[1], 1).to(device)\n",
    "            # mut1 = torch.cat((mut1, zero_row), dim=-1)\n",
    "            #处理mut0，mut1，去除0，找到位置，接入全局变量，01拼接在模型里\n",
    "            # mut0 = mut0[:, 1:, :]\n",
    "            if mut0.shape[1] < 2*len_half_protein_used:\n",
    "                mut0 = torch.cat((mut0.mean(dim=1, keepdim=True),mut0),dim=1)\n",
    "                mut0_mask = mut0_mask\n",
    "                mut1 = torch.cat((mut1.mean(dim=1, keepdim=True),mut1),dim=1)\n",
    "                mut1_mask = mut1_mask\n",
    "            else:\n",
    "                result = torch.randn(mut0.shape[0],2*len_half_protein_used,mut0.shape[2])\n",
    "                result_padding = (np.random.randint(0, 2, size=(mut0.shape[0], 2*len_half_protein_used)) == 1)\n",
    "\n",
    "                for ia in range(len(positions)):\n",
    "                    position = int(positions[ia])\n",
    "                    if position - len_half_protein_used < 0 :\n",
    "                        result[ia, :, :] = mut0[ia,:2 * len_half_protein_used,:]\n",
    "                        result_padding[ia, :] = mut0_mask[ia, : 2 * len_half_protein_used].cpu()\n",
    "                    elif position + len_half_protein_used > mut0.shape[1] :\n",
    "                        result[ia, :, :] = mut0[ia,-2 * len_half_protein_used:,:]\n",
    "                        result_padding[ia, :] = mut0_mask[ia, -2 * len_half_protein_used].cpu()\n",
    "                    else:\n",
    "                        result[ia, :, :] = mut0[ia,position - len_half_protein_used:position + len_half_protein_used,:]\n",
    "                        result_padding[ia, :] = mut0_mask[ia, position - len_half_protein_used : position + len_half_protein_used].cpu()\n",
    "\n",
    "                mut0 = torch.cat((mut0.mean(dim=1, keepdim=True).cpu(), result),dim=1)\n",
    "                mut0_mask = result_padding\n",
    "                mut0_mask = np.concatenate([np.full((mut0_mask.shape[0], 1), False), mut0_mask], axis=1)\n",
    "                \n",
    "                \n",
    "                \n",
    "                result = torch.randn(mut1.shape[0],2*len_half_protein_used,mut1.shape[2])\n",
    "                result_padding = (np.random.randint(0, 2, size=(mut1.shape[0], 2*len_half_protein_used)) == 1)\n",
    "                for ia in range(len(positions)):\n",
    "                    position = int(positions[ia])\n",
    "                    if position - len_half_protein_used < 0 :\n",
    "                        result[ia, :, :] = mut1[ia,:2 * len_half_protein_used,:]\n",
    "                        result_padding[ia, :] = mut1_mask[ia , : 2 * len_half_protein_used].cpu()\n",
    "                    elif position + len_half_protein_used > mut1.shape[1] :\n",
    "                        result[ia, :, :] = mut1[ia,-2 * len_half_protein_used : ,:]\n",
    "                        result_padding[ia, :] = mut1_mask[ia , -2 * len_half_protein_used].cpu()\n",
    "                    else:\n",
    "                        # print(i)\n",
    "                        result[ia, :, :] = mut1[ia,position - len_half_protein_used:position + len_half_protein_used,:]\n",
    "                        result_padding[ia, :] = mut1_mask[ia, position - len_half_protein_used : position + len_half_protein_used].cpu()\n",
    "                mut1 = torch.cat((mut1.mean(dim=1, keepdim=True).cpu(), result),dim=1)\n",
    "                mut1_mask = result_padding\n",
    "                mut1_mask = np.concatenate([np.full((mut1_mask.shape[0], 1), False), mut1_mask], axis=1)\n",
    "                \n",
    "                \n",
    "            mut0_mask = torch.from_numpy(mut0_mask)\n",
    "            mut0_mask = mut0_mask.to(device)\n",
    "            mut1_mask = torch.from_numpy(mut1_mask)\n",
    "            mut1_mask = mut1_mask.to(device)\n",
    "            mut0 = mut0.to(device)\n",
    "            mut1 = mut1.to(device)\n",
    "            par0 = par0.to(device)\n",
    "            mut0 = mut0 + position_embedding[:mut0.shape[1] , : ]\n",
    "            mut1 = mut1 + position_embedding[2 * len_half_protein_used + 1 : 2 * len_half_protein_used + 1 + mut1.shape[1] , : ]\n",
    "            x = model(mut0, mut1, par0,torch.cat((mut0_mask,mut1_mask),dim=1),par0_mask, class_weights,batch[\"label\"])\n",
    "            _, predicted = torch.max(x, 1)\n",
    "            label_pred = label_pred + predicted.tolist()\n",
    "            label_need = label_need + batch['label'].to_list()\n",
    "\n",
    "    accuracy = accuracy_score(label_need, label_pred)\n",
    "\n",
    "    acc_test.append(accuracy)\n",
    "    model.train()\n",
    "    with open('test_result_mirror_multi.json', 'w') as file:\n",
    "        json.dump(acc_test, file)\n",
    "    with open('train_result_mirror_multi.json', 'w') as file:\n",
    "        json.dump(acc_train, file)\n",
    "    if accuracy > 0.85:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4809f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7, 21, 1280]),\n",
       " torch.Size([7, 21, 1280]),\n",
       " torch.Size([7, 593, 1280]),\n",
       " torch.Size([7, 42]),\n",
       " torch.Size([7, 593]),\n",
       " array([1.13315807, 1.38630794, 0.59707564, 1.38630794]),\n",
       " 0    0\n",
       " 1    2\n",
       " 2    2\n",
       " 3    2\n",
       " 4    1\n",
       " 5    2\n",
       " 6    3\n",
       " Name: label, dtype: int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mut0.shape, mut1.shape, par0.shape, torch.cat((mut0_mask,mut1_mask),dim=1).shape, par0_mask.shape, class_weights, batch[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b393b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 4])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "model = Sum_model(device)\n",
    "model.to(device)\n",
    "mut0 = torch.randn(7, 21, 1280).to(device)\n",
    "mut1 = torch.randn(7, 21, 1280).to(device)\n",
    "par0 = torch.randn(7, 593, 1280).to(device)\n",
    "mut0_mask = torch.randint(0, 2, (7, 21), dtype=torch.bool).to(device)  # 随机生成布尔值\n",
    "mut1_mask = torch.randint(0, 2, (7, 21), dtype=torch.bool).to(device)  # 随机生成布尔值\n",
    "par0_mask = torch.randint(0, 2, (7, 593), dtype=torch.bool).to(device)  # 随机生成布尔值\n",
    "model(mut0, mut1, par0,torch.cat((mut0_mask,mut1_mask),dim=1),par0_mask, None,None).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b2cccd45",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/root/anaconda3/envs/lyk_mippi/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in _worker\n    output = module(*input, **kwargs)\n  File \"/root/anaconda3/envs/lyk_mippi/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/anaconda3/envs/lyk_mippi/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_3408047/837373959.py\", line 15, in forward\n    par_padding_mask = torch.cat((false_column, par_padding_mask), dim=1)\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument tensors in method wrapper_CUDA_cat)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m par0_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, (\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m593\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# 随机生成布尔值\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 在模型前向传播时使用 DataParallel\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmut0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmut1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpar0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmut0_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmut1_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpar0_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/anaconda3/envs/lyk_mippi/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lyk_mippi/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lyk_mippi/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:185\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/lyk_mippi/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:200\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lyk_mippi/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:110\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    108\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 110\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/lyk_mippi/lib/python3.8/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/root/anaconda3/envs/lyk_mippi/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in _worker\n    output = module(*input, **kwargs)\n  File \"/root/anaconda3/envs/lyk_mippi/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/root/anaconda3/envs/lyk_mippi/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_3408047/837373959.py\", line 15, in forward\n    par_padding_mask = torch.cat((false_column, par_padding_mask), dim=1)\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument tensors in method wrapper_CUDA_cat)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定义设备和模型\n",
    "device = torch.device('cuda:0')\n",
    "model = Sum_model(device)\n",
    "model = torch.nn.DataParallel(model, device_ids=[0, 1, 2])  # 使用 DataParallel 在多张卡上训练\n",
    "model.to(device)\n",
    "\n",
    "# 生成随机输入数据\n",
    "mut0 = torch.randn(7, 21, 1280).to(device)\n",
    "mut1 = torch.randn(7, 21, 1280).to(device)\n",
    "par0 = torch.randn(7, 593, 1280).to(device)\n",
    "mut0_mask = torch.randint(0, 2, (7, 21), dtype=torch.bool).to(device)  # 随机生成布尔值\n",
    "mut1_mask = torch.randint(0, 2, (7, 21), dtype=torch.bool).to(device)  # 随机生成布尔值\n",
    "par0_mask = torch.randint(0, 2, (7, 593), dtype=torch.bool).to(device)  # 随机生成布尔值\n",
    "\n",
    "# 在模型前向传播时使用 DataParallel\n",
    "output_shape = model(mut0, mut1, par0, torch.cat((mut0_mask, mut1_mask), dim=1), par0_mask, None, None).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ab6b3f6-0077-4947-9f2b-4f52e13fc32c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test_fold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mx_test_fold\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_test_fold' is not defined"
     ]
    }
   ],
   "source": [
    "x_test_fold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aeae9423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ...,  True,  True,  True],\n",
       "        [False, False, False,  ...,  True,  True,  True],\n",
       "        [False, False, False,  ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [False, False, False,  ...,  True,  True,  True],\n",
       "        [False, False, False,  ...,  True,  True,  True],\n",
       "        [False, False, False,  ...,  True,  True,  True]], device='cuda:3')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mut1_mask = np.concatenate([np.full((mut1_mask.shape[0], 1), False), mut1_mask], axis=1)\n",
    "par0_mask.shape\n",
    "false_column = torch.zeros(par0_mask.size(0), 1, dtype=torch.bool).to(device)\n",
    "# 在第一列前添加一排 False\n",
    "par_padding_mask = torch.cat((false_column, par0_mask), dim=1)\n",
    "par_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba35f33c-242d-4829-9168-fc6d7eb117e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 训练集的准确率列表\n",
    "train_accuracy = acc_train\n",
    "\n",
    "# 测试集的准确率列表\n",
    "test_accuracy = acc_test\n",
    "\n",
    "# 对应的epochs或时间步列表\n",
    "epochs = range(len(acc_test))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_accuracy, marker='o', label='Train Accuracy')\n",
    "plt.plot(epochs, test_accuracy, marker='s', label='Test Accuracy')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Testing Accuracy over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cedb87-b490-40d5-8fe1-9138b3b88edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 损失值列表\n",
    "loss_values = loss_range[-1600:]\n",
    "\n",
    "# 对应的epochs或时间步列表\n",
    "epochs = range(len(loss_values))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, loss_values, color='r')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Value over Epochs')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6da0f9-0b3b-484c-8691-3409287fb169",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_need, label_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58780350-6bc4-460f-b900-0e86a53e24a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_need_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3306dccf-a3a7-4100-aa5f-e7423051c6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoc in range(40):\n",
    "    label_need_train = []\n",
    "    label_pred_train = []\n",
    "    label_need = []\n",
    "    label_pred = []\n",
    "    i = 0\n",
    "    loss_onebatch = 0\n",
    "    x_train_fold = x_train_fold.sample(frac=1).reset_index(drop=True)\n",
    "    data_reader = PandasDataReader(x_train_fold, batch_size=batch_size, shuffle=True)\n",
    "    num_epochs = len(x_train_fold)\n",
    "    for batch in data_reader:\n",
    "        if False:\n",
    "            loss = model([batch[\"mut0\"]],[ batch[\"mut1\"]], [batch[\"par0\"]], [max(int(batch[\"Feature range(s)\"][0].split(\"-\")[0])-25,0),max(int(batch[\"Feature range(s)\"][0].split(\"-\")[0])-25,0)+50],class_weights,[batch[\"label\"]],device) # 三个序列和需要的突变采样位置，如突变从80号开始就是[80-25，80+25]\n",
    "            loss_onebatch = loss_onebatch + loss.item()\n",
    "        else:\n",
    "            positions = batch[\"position\"].tolist()\n",
    "            # print([sequence for sequence in batch[\"mut0\"]][0])\n",
    "            mut0,mut0_mask = esm_model.encode([sequence for sequence in batch[\"mut0\"]])\n",
    "            mut1,mut1_mask = esm_model.encode([sequence for sequence in batch[\"mut1\"]])\n",
    "            par0,par0_mask = esm_model.encode([sequence for sequence in batch[\"par0\"]])\n",
    "            #类似位置编码，标记突变前后的\n",
    "            zero_row = torch.zeros(mut0.shape[0], mut0.shape[1], 1).to(device)\n",
    "            mut0 = torch.cat((mut0, zero_row), dim=-1)\n",
    "            zero_row = torch.ones(mut1.shape[0], mut1.shape[1], 1).to(device)\n",
    "            mut1 = torch.cat((mut1, zero_row), dim=-1)\n",
    "            #处理mut0，mut1，去除0，找到位置，接入全局变量，01拼接在模型里\n",
    "            # mut0 = mut0[:, 1:, :]\n",
    "            if mut0.shape[1] < 2*len_half_protein_used:\n",
    "                mut0 = torch.cat((mut0.mean(dim=1, keepdim=True),mut0),dim=1)\n",
    "                mut0_mask = mut0_mask\n",
    "                mut1 = torch.cat((mut1.mean(dim=1, keepdim=True),mut1),dim=1)\n",
    "                mut1_mask = mut1_mask\n",
    "            else:\n",
    "                result = torch.randn(mut0.shape[0],2*len_half_protein_used,mut0.shape[2])\n",
    "                result_padding = (np.random.randint(0, 2, size=(mut0.shape[0], 2*len_half_protein_used)) == 1)\n",
    "\n",
    "                for ia in range(len(positions)):\n",
    "                    position = int(positions[ia])\n",
    "                    if position - len_half_protein_used < 0 :\n",
    "                        result[ia, :, :] = mut0[ia,:2 * len_half_protein_used,:]\n",
    "                        result_padding[ia, :] = mut0_mask[ia,1:1+2 * len_half_protein_used].cpu()\n",
    "                    elif position + len_half_protein_used > mut0.shape[1] - 1:\n",
    "                        result[ia, :, :] = mut0[ia,-2 * len_half_protein_used:,:]\n",
    "                        result_padding[ia, :] = mut0_mask[ia, -2 * len_half_protein_used].cpu()\n",
    "                    else:\n",
    "                        result[ia, :, :] = mut0[ia,position - len_half_protein_used:position + len_half_protein_used,:]\n",
    "                        result_padding[ia, :] = mut0_mask[ia,1+ position - len_half_protein_used:1 + position + len_half_protein_used].cpu()\n",
    "\n",
    "                mut0 = torch.cat((mut0.mean(dim=1, keepdim=True).cpu(), result),dim=1)\n",
    "                mut0_mask = result_padding\n",
    "                mut0_mask = np.concatenate([np.full((mut0_mask.shape[0], 1), False), mut0_mask], axis=1)\n",
    "                \n",
    "                \n",
    "                \n",
    "                result = torch.randn(mut1.shape[0],2*len_half_protein_used,mut1.shape[2])\n",
    "                result_padding = (np.random.randint(0, 2, size=(mut1.shape[0], 2*len_half_protein_used)) == 1)\n",
    "                for ia in range(len(positions)):\n",
    "                    position = int(positions[ia])\n",
    "                    if position - len_half_protein_used < 0 :\n",
    "                        result[ia, :, :] = mut1[ia,:2 * len_half_protein_used,:]\n",
    "                        result_padding[ia, :] = mut1_mask[ia,1:1+2 * len_half_protein_used].cpu()\n",
    "                    elif position + len_half_protein_used > mut1.shape[1] - 1:\n",
    "                        result[ia, :, :] = mut1[ia,-2 * len_half_protein_used : ,:]\n",
    "                        result_padding[ia, :] = mut1_mask[ia, -2 * len_half_protein_used].cpu()\n",
    "                    else:\n",
    "                        # print(i)\n",
    "                        result[ia, :, :] = mut1[ia,position - len_half_protein_used:position + len_half_protein_used,:]\n",
    "                        result_padding[ia, :] = mut1_mask[ia,1+ position - len_half_protein_used:1 + position + len_half_protein_used].cpu()\n",
    "                mut1 = torch.cat((mut1.mean(dim=1, keepdim=True).cpu(), result),dim=1)\n",
    "                mut1_mask = result_padding\n",
    "                mut1_mask = np.concatenate([np.full((mut1_mask.shape[0], 1), False), mut1_mask], axis=1)\n",
    "                \n",
    "                \n",
    "            mut0_mask = torch.from_numpy(mut0_mask)\n",
    "            mut0_mask = mut0_mask.to(device)\n",
    "            mut1_mask = torch.from_numpy(mut1_mask)\n",
    "            mut1_mask = mut1_mask.to(device)\n",
    "            mut0 = mut0.to(device)\n",
    "            mut1 = mut1.to(device)\n",
    "            par0 = par0.to(device)\n",
    "            # par0_mask = torch.from_numpy(par0_mask)\n",
    "            # for position in batch[\"Feature range(s)\"]:\n",
    "            #     positions.append([max(int(position[0].split(\"-\")[0])-25,0),max(int(position[0].split(\"-\")[0])-25,0)+50])\n",
    "            out = model(mut0, mut1, par0,torch.cat((mut0_mask,mut1_mask),dim=1),par0_mask, class_weights,batch[\"label\"])\n",
    "            loss = model_loss(batch[\"label\"], out,class_weights)\n",
    "            _, predicted_labels = torch.max(out, 1)\n",
    "            # break\n",
    "            # loss = ghm_loss(out, batch[\"label\"].to_list())\n",
    "            # loss_onebatch = loss_onebatch + loss.item()\n",
    "            loss_range.append(loss.item())\n",
    "            # print(loss.item())\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(\"done\")\n",
    "        label_need_train = label_need_train + batch[\"label\"].to_list()\n",
    "        label_pred_train = label_pred_train + predicted_labels.tolist()\n",
    "        if (i) % batch_size*20 == 0:\n",
    "            clear_output()\n",
    "            print(f' [{i}/{num_epochs}], Loss: {loss}')\n",
    "\n",
    "        i = i+batch_size\n",
    "    accuracy = accuracy_score(label_need_train,label_pred_train)\n",
    "    acc_train.append(accuracy)\n",
    "    torch.save(model.state_dict(), 'model_params_copy3.pth')\n",
    "    model.eval()\n",
    "    data_reader = PandasDataReader(x_test_fold, batch_size=10, shuffle=True)\n",
    "    num_epochs = len(x_train_fold)\n",
    "    print(\"testing\",epoc)\n",
    "    for batch in data_reader:\n",
    "        if False:\n",
    "            loss = model([batch[\"mut0\"]],[ batch[\"mut1\"]], [batch[\"par0\"]], [max(int(batch[\"Feature range(s)\"][0].split(\"-\")[0])-25,0),max(int(batch[\"Feature range(s)\"][0].split(\"-\")[0])-25,0)+50],class_weights,[batch[\"label\"]],device) # 三个序列和需要的突变采样位置，如突变从80号开始就是[80-25，80+25]\n",
    "            loss_onebatch = loss_onebatch + loss.item()\n",
    "        else:\n",
    "            positions = batch[\"position\"].tolist()\n",
    "            # print([sequence for sequence in batch[\"mut0\"]][0])\n",
    "            mut0,mut0_mask = esm_model.encode([sequence for sequence in batch[\"mut0\"]])\n",
    "            mut1,mut1_mask = esm_model.encode([sequence for sequence in batch[\"mut1\"]])\n",
    "            par0,par0_mask = esm_model.encode([sequence for sequence in batch[\"par0\"]])\n",
    "            zero_row = torch.zeros(mut0.shape[0], mut0.shape[1], 1).to(device)\n",
    "            mut0 = torch.cat((mut0, zero_row), dim=-1)\n",
    "            zero_row = torch.ones(mut1.shape[0], mut1.shape[1], 1).to(device)\n",
    "            mut1 = torch.cat((mut1, zero_row), dim=-1)\n",
    "            #处理mut0，mut1，去除0，找到位置，接入全局变量，01拼接在模型里\n",
    "            # mut0 = mut0[:, 1:, :]\n",
    "            if mut0.shape[1] < 2*len_half_protein_used:\n",
    "                mut0 = torch.cat((mut0.mean(dim=1, keepdim=True),mut0),dim=1)\n",
    "                mut0_mask = mut0_mask\n",
    "                mut1 = torch.cat((mut1.mean(dim=1, keepdim=True),mut1),dim=1)\n",
    "                mut1_mask = mut1_mask\n",
    "            else:\n",
    "                result = torch.randn(mut0.shape[0],2*len_half_protein_used,mut0.shape[2])\n",
    "                result_padding = (np.random.randint(0, 2, size=(mut0.shape[0], 2*len_half_protein_used)) == 1)\n",
    "\n",
    "                for ia in range(len(positions)):\n",
    "                    position = int(positions[ia])\n",
    "                    if position - len_half_protein_used < 0 :\n",
    "                        result[ia, :, :] = mut0[ia,:2 * len_half_protein_used,:]\n",
    "                        result_padding[ia, :] = mut0_mask[ia,1:1+2 * len_half_protein_used].cpu()\n",
    "                    elif position + len_half_protein_used > mut0.shape[1] - 1:\n",
    "                        result[ia, :, :] = mut0[ia,-2 * len_half_protein_used:,:]\n",
    "                        result_padding[ia, :] = mut0_mask[ia, -2 * len_half_protein_used].cpu()\n",
    "                    else:\n",
    "                        result[ia, :, :] = mut0[ia,position - len_half_protein_used:position + len_half_protein_used,:]\n",
    "                        result_padding[ia, :] = mut0_mask[ia,1+ position - len_half_protein_used:1 + position + len_half_protein_used].cpu()\n",
    "\n",
    "                mut0 = torch.cat((mut0.mean(dim=1, keepdim=True).cpu(), result),dim=1)\n",
    "                mut0_mask = result_padding\n",
    "                mut0_mask = np.concatenate([np.full((mut0_mask.shape[0], 1), False), mut0_mask], axis=1)\n",
    "                \n",
    "                \n",
    "                \n",
    "                result = torch.randn(mut1.shape[0],2*len_half_protein_used,mut1.shape[2])\n",
    "                result_padding = (np.random.randint(0, 2, size=(mut1.shape[0], 2*len_half_protein_used)) == 1)\n",
    "                for ia in range(len(positions)):\n",
    "                    position = int(positions[ia])\n",
    "                    if position - len_half_protein_used < 0 :\n",
    "                        result[ia, :, :] = mut1[ia,:2 * len_half_protein_used,:]\n",
    "                        result_padding[ia, :] = mut1_mask[ia,1:1+2 * len_half_protein_used].cpu()\n",
    "                    elif position + len_half_protein_used > mut1.shape[1] - 1:\n",
    "                        result[ia, :, :] = mut1[ia,-2 * len_half_protein_used : ,:]\n",
    "                        result_padding[ia, :] = mut1_mask[ia, -2 * len_half_protein_used].cpu()\n",
    "                    else:\n",
    "                        # print(i)\n",
    "                        result[ia, :, :] = mut1[ia,position - len_half_protein_used:position + len_half_protein_used,:]\n",
    "                        result_padding[ia, :] = mut1_mask[ia,1+ position - len_half_protein_used:1 + position + len_half_protein_used].cpu()\n",
    "                mut1 = torch.cat((mut1.mean(dim=1, keepdim=True).cpu(), result),dim=1)\n",
    "                mut1_mask = result_padding\n",
    "                mut1_mask = np.concatenate([np.full((mut1_mask.shape[0], 1), False), mut1_mask], axis=1)\n",
    "                \n",
    "                \n",
    "            mut0_mask = torch.from_numpy(mut0_mask)\n",
    "            mut0_mask = mut0_mask.to(device)\n",
    "            mut1_mask = torch.from_numpy(mut1_mask)\n",
    "            mut1_mask = mut1_mask.to(device)\n",
    "            mut0 = mut0.to(device)\n",
    "            mut1 = mut1.to(device)\n",
    "            par0 = par0.to(device)\n",
    "            x = model(mut0, mut1, par0,torch.cat((mut0_mask,mut1_mask),dim=1),par0_mask, class_weights,batch[\"label\"])\n",
    "            _, predicted = torch.max(x, 1)\n",
    "            label_pred = label_pred + predicted.tolist()\n",
    "            label_need = label_need + batch['label'].to_list()\n",
    "    accuracy = accuracy_score(label_need, label_pred)\n",
    "    acc_test.append(accuracy)\n",
    "    model.train()\n",
    "    with open('test_result_copy3.json', 'w') as file:\n",
    "        json.dump(acc_test, file)\n",
    "    with open('train_result_copy3.json', 'w') as file:\n",
    "        json.dump(acc_train, file)\n",
    "    if accuracy > 0.8:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a0d3610-178f-4d81-9dee-90e44b70afe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3098, 3098)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_need),len(label_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77b4d724",
   "metadata": {},
   "outputs": [],
   "source": [
    "    with open('label_need_base.json', 'w') as file:\n",
    "        json.dump(label_need, file)\n",
    "    with open('label_pred_base.json', 'w') as file:\n",
    "        json.dump(label_pred, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd74876-5560-4d73-9679-a0ca2ceabd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba2137c7-6fd6-45fd-9508-08d0f52d27e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_fold.to_csv('data/x_train_fold_mirror_base.csv')\n",
    "x_test_fold.to_csv('data/x_test_fold_mirror_base.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
